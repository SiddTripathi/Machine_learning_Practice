{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r'D:\\college\\sem2\\Data Mining and Machine learning\\Assignment_Data_mining\\Forest.xlsx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawdata = pd.read_excel(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>b1</th>\n",
       "      <th>b2</th>\n",
       "      <th>b3</th>\n",
       "      <th>b4</th>\n",
       "      <th>b5</th>\n",
       "      <th>b6</th>\n",
       "      <th>b7</th>\n",
       "      <th>b8</th>\n",
       "      <th>b9</th>\n",
       "      <th>...</th>\n",
       "      <th>pred_minus_obs_H_b9</th>\n",
       "      <th>pred_minus_obs_S_b1</th>\n",
       "      <th>pred_minus_obs_S_b2</th>\n",
       "      <th>pred_minus_obs_S_b3</th>\n",
       "      <th>pred_minus_obs_S_b4</th>\n",
       "      <th>pred_minus_obs_S_b5</th>\n",
       "      <th>pred_minus_obs_S_b6</th>\n",
       "      <th>pred_minus_obs_S_b7</th>\n",
       "      <th>pred_minus_obs_S_b8</th>\n",
       "      <th>pred_minus_obs_S_b9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>d</td>\n",
       "      <td>39</td>\n",
       "      <td>36</td>\n",
       "      <td>57</td>\n",
       "      <td>91</td>\n",
       "      <td>59</td>\n",
       "      <td>101</td>\n",
       "      <td>93</td>\n",
       "      <td>27</td>\n",
       "      <td>60</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.36</td>\n",
       "      <td>-18.41</td>\n",
       "      <td>-1.88</td>\n",
       "      <td>-6.43</td>\n",
       "      <td>-21.03</td>\n",
       "      <td>-1.60</td>\n",
       "      <td>-6.18</td>\n",
       "      <td>-22.50</td>\n",
       "      <td>-5.20</td>\n",
       "      <td>-7.86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>h</td>\n",
       "      <td>84</td>\n",
       "      <td>30</td>\n",
       "      <td>57</td>\n",
       "      <td>112</td>\n",
       "      <td>51</td>\n",
       "      <td>98</td>\n",
       "      <td>92</td>\n",
       "      <td>26</td>\n",
       "      <td>62</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.26</td>\n",
       "      <td>-16.27</td>\n",
       "      <td>-1.95</td>\n",
       "      <td>-6.25</td>\n",
       "      <td>-18.79</td>\n",
       "      <td>-1.99</td>\n",
       "      <td>-6.18</td>\n",
       "      <td>-23.41</td>\n",
       "      <td>-8.87</td>\n",
       "      <td>-10.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>s</td>\n",
       "      <td>53</td>\n",
       "      <td>25</td>\n",
       "      <td>49</td>\n",
       "      <td>99</td>\n",
       "      <td>51</td>\n",
       "      <td>93</td>\n",
       "      <td>84</td>\n",
       "      <td>26</td>\n",
       "      <td>58</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.46</td>\n",
       "      <td>-15.92</td>\n",
       "      <td>-1.79</td>\n",
       "      <td>-4.64</td>\n",
       "      <td>-17.73</td>\n",
       "      <td>-0.48</td>\n",
       "      <td>-4.69</td>\n",
       "      <td>-19.97</td>\n",
       "      <td>-4.10</td>\n",
       "      <td>-7.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>s</td>\n",
       "      <td>59</td>\n",
       "      <td>26</td>\n",
       "      <td>49</td>\n",
       "      <td>103</td>\n",
       "      <td>47</td>\n",
       "      <td>92</td>\n",
       "      <td>82</td>\n",
       "      <td>25</td>\n",
       "      <td>56</td>\n",
       "      <td>...</td>\n",
       "      <td>2.68</td>\n",
       "      <td>-13.77</td>\n",
       "      <td>-2.53</td>\n",
       "      <td>-6.34</td>\n",
       "      <td>-22.03</td>\n",
       "      <td>-2.34</td>\n",
       "      <td>-6.60</td>\n",
       "      <td>-27.10</td>\n",
       "      <td>-7.99</td>\n",
       "      <td>-10.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>d</td>\n",
       "      <td>57</td>\n",
       "      <td>49</td>\n",
       "      <td>66</td>\n",
       "      <td>103</td>\n",
       "      <td>64</td>\n",
       "      <td>106</td>\n",
       "      <td>114</td>\n",
       "      <td>28</td>\n",
       "      <td>59</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.94</td>\n",
       "      <td>-21.74</td>\n",
       "      <td>-1.64</td>\n",
       "      <td>-4.62</td>\n",
       "      <td>-23.74</td>\n",
       "      <td>-0.85</td>\n",
       "      <td>-5.50</td>\n",
       "      <td>-22.83</td>\n",
       "      <td>-2.74</td>\n",
       "      <td>-5.84</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  class  b1  b2  b3   b4  b5   b6   b7  b8  b9  ...  pred_minus_obs_H_b9  \\\n",
       "0    d   39  36  57   91  59  101   93  27  60  ...                -2.36   \n",
       "1    h   84  30  57  112  51   98   92  26  62  ...                -2.26   \n",
       "2    s   53  25  49   99  51   93   84  26  58  ...                -1.46   \n",
       "3    s   59  26  49  103  47   92   82  25  56  ...                 2.68   \n",
       "4    d   57  49  66  103  64  106  114  28  59  ...                -2.94   \n",
       "\n",
       "   pred_minus_obs_S_b1  pred_minus_obs_S_b2  pred_minus_obs_S_b3  \\\n",
       "0               -18.41                -1.88                -6.43   \n",
       "1               -16.27                -1.95                -6.25   \n",
       "2               -15.92                -1.79                -4.64   \n",
       "3               -13.77                -2.53                -6.34   \n",
       "4               -21.74                -1.64                -4.62   \n",
       "\n",
       "   pred_minus_obs_S_b4  pred_minus_obs_S_b5  pred_minus_obs_S_b6  \\\n",
       "0               -21.03                -1.60                -6.18   \n",
       "1               -18.79                -1.99                -6.18   \n",
       "2               -17.73                -0.48                -4.69   \n",
       "3               -22.03                -2.34                -6.60   \n",
       "4               -23.74                -0.85                -5.50   \n",
       "\n",
       "   pred_minus_obs_S_b7  pred_minus_obs_S_b8  pred_minus_obs_S_b9  \n",
       "0               -22.50                -5.20                -7.86  \n",
       "1               -23.41                -8.87               -10.83  \n",
       "2               -19.97                -4.10                -7.07  \n",
       "3               -27.10                -7.99               -10.81  \n",
       "4               -22.83                -2.74                -5.84  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rawdata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_target=rawdata.iloc[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrow, ncol = rawdata.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawdata['class']= label_encoder.fit_transform(rawdata['class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>b1</th>\n",
       "      <th>b2</th>\n",
       "      <th>b3</th>\n",
       "      <th>b4</th>\n",
       "      <th>b5</th>\n",
       "      <th>b6</th>\n",
       "      <th>b7</th>\n",
       "      <th>b8</th>\n",
       "      <th>b9</th>\n",
       "      <th>...</th>\n",
       "      <th>pred_minus_obs_H_b9</th>\n",
       "      <th>pred_minus_obs_S_b1</th>\n",
       "      <th>pred_minus_obs_S_b2</th>\n",
       "      <th>pred_minus_obs_S_b3</th>\n",
       "      <th>pred_minus_obs_S_b4</th>\n",
       "      <th>pred_minus_obs_S_b5</th>\n",
       "      <th>pred_minus_obs_S_b6</th>\n",
       "      <th>pred_minus_obs_S_b7</th>\n",
       "      <th>pred_minus_obs_S_b8</th>\n",
       "      <th>pred_minus_obs_S_b9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>39</td>\n",
       "      <td>36</td>\n",
       "      <td>57</td>\n",
       "      <td>91</td>\n",
       "      <td>59</td>\n",
       "      <td>101</td>\n",
       "      <td>93</td>\n",
       "      <td>27</td>\n",
       "      <td>60</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.36</td>\n",
       "      <td>-18.41</td>\n",
       "      <td>-1.88</td>\n",
       "      <td>-6.43</td>\n",
       "      <td>-21.03</td>\n",
       "      <td>-1.60</td>\n",
       "      <td>-6.18</td>\n",
       "      <td>-22.50</td>\n",
       "      <td>-5.20</td>\n",
       "      <td>-7.86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>84</td>\n",
       "      <td>30</td>\n",
       "      <td>57</td>\n",
       "      <td>112</td>\n",
       "      <td>51</td>\n",
       "      <td>98</td>\n",
       "      <td>92</td>\n",
       "      <td>26</td>\n",
       "      <td>62</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.26</td>\n",
       "      <td>-16.27</td>\n",
       "      <td>-1.95</td>\n",
       "      <td>-6.25</td>\n",
       "      <td>-18.79</td>\n",
       "      <td>-1.99</td>\n",
       "      <td>-6.18</td>\n",
       "      <td>-23.41</td>\n",
       "      <td>-8.87</td>\n",
       "      <td>-10.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>53</td>\n",
       "      <td>25</td>\n",
       "      <td>49</td>\n",
       "      <td>99</td>\n",
       "      <td>51</td>\n",
       "      <td>93</td>\n",
       "      <td>84</td>\n",
       "      <td>26</td>\n",
       "      <td>58</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.46</td>\n",
       "      <td>-15.92</td>\n",
       "      <td>-1.79</td>\n",
       "      <td>-4.64</td>\n",
       "      <td>-17.73</td>\n",
       "      <td>-0.48</td>\n",
       "      <td>-4.69</td>\n",
       "      <td>-19.97</td>\n",
       "      <td>-4.10</td>\n",
       "      <td>-7.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>59</td>\n",
       "      <td>26</td>\n",
       "      <td>49</td>\n",
       "      <td>103</td>\n",
       "      <td>47</td>\n",
       "      <td>92</td>\n",
       "      <td>82</td>\n",
       "      <td>25</td>\n",
       "      <td>56</td>\n",
       "      <td>...</td>\n",
       "      <td>2.68</td>\n",
       "      <td>-13.77</td>\n",
       "      <td>-2.53</td>\n",
       "      <td>-6.34</td>\n",
       "      <td>-22.03</td>\n",
       "      <td>-2.34</td>\n",
       "      <td>-6.60</td>\n",
       "      <td>-27.10</td>\n",
       "      <td>-7.99</td>\n",
       "      <td>-10.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>57</td>\n",
       "      <td>49</td>\n",
       "      <td>66</td>\n",
       "      <td>103</td>\n",
       "      <td>64</td>\n",
       "      <td>106</td>\n",
       "      <td>114</td>\n",
       "      <td>28</td>\n",
       "      <td>59</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.94</td>\n",
       "      <td>-21.74</td>\n",
       "      <td>-1.64</td>\n",
       "      <td>-4.62</td>\n",
       "      <td>-23.74</td>\n",
       "      <td>-0.85</td>\n",
       "      <td>-5.50</td>\n",
       "      <td>-22.83</td>\n",
       "      <td>-2.74</td>\n",
       "      <td>-5.84</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   class  b1  b2  b3   b4  b5   b6   b7  b8  b9  ...  pred_minus_obs_H_b9  \\\n",
       "0      0  39  36  57   91  59  101   93  27  60  ...                -2.36   \n",
       "1      1  84  30  57  112  51   98   92  26  62  ...                -2.26   \n",
       "2      3  53  25  49   99  51   93   84  26  58  ...                -1.46   \n",
       "3      3  59  26  49  103  47   92   82  25  56  ...                 2.68   \n",
       "4      0  57  49  66  103  64  106  114  28  59  ...                -2.94   \n",
       "\n",
       "   pred_minus_obs_S_b1  pred_minus_obs_S_b2  pred_minus_obs_S_b3  \\\n",
       "0               -18.41                -1.88                -6.43   \n",
       "1               -16.27                -1.95                -6.25   \n",
       "2               -15.92                -1.79                -4.64   \n",
       "3               -13.77                -2.53                -6.34   \n",
       "4               -21.74                -1.64                -4.62   \n",
       "\n",
       "   pred_minus_obs_S_b4  pred_minus_obs_S_b5  pred_minus_obs_S_b6  \\\n",
       "0               -21.03                -1.60                -6.18   \n",
       "1               -18.79                -1.99                -6.18   \n",
       "2               -17.73                -0.48                -4.69   \n",
       "3               -22.03                -2.34                -6.60   \n",
       "4               -23.74                -0.85                -5.50   \n",
       "\n",
       "   pred_minus_obs_S_b7  pred_minus_obs_S_b8  pred_minus_obs_S_b9  \n",
       "0               -22.50                -5.20                -7.86  \n",
       "1               -23.41                -8.87               -10.83  \n",
       "2               -19.97                -4.10                -7.07  \n",
       "3               -27.10                -7.99               -10.81  \n",
       "4               -22.83                -2.74                -5.84  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rawdata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictors=rawdata.iloc[:,1:ncol]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "target=rawdata.iloc[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "\n",
    "pred_train, pred_test, tar_train, tar_test  = train_test_split(predictors,target,test_size=.3,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_threshold=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr = dict()                  # store false positive rate in a dictionary object\n",
    "tpr = dict()                  # likewise, store the true positive rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors=['blue','green','red']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################\n",
    "#----Decision Tree Classifier------\n",
    "##########################################\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score, recall_score, auc\n",
    "from sklearn.metrics import precision_score, recall_score, auc ,confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is the output for decision tree classifier\n",
      "Accuracy score of our model with Decision Tree: 0.7961783439490446\n",
      "Precision score of our model with Decision Tree : 0.7961783439490446\n",
      "Recall score of our model with Decision Tree : 0.7961783439490446\n"
     ]
    }
   ],
   "source": [
    "print('Below is the output for decision tree classifier')\n",
    "\n",
    "\n",
    "classifier = DecisionTreeClassifier(max_depth=3)  # configure the classifier\n",
    "classifier = classifier.fit(pred_train, tar_train)  # train a decision tree model\n",
    "predictionsDT = classifier.predict(pred_test)  # deploy model and make predictions on test set\n",
    "probDT = classifier.predict_proba(pred_test)  # obtain probability scores for each sample in test set\n",
    "print(\"Accuracy score of our model with Decision Tree:\", accuracy_score(tar_test, predictionsDT))\n",
    "precision = precision_score(y_true=tar_test, y_pred=predictionsDT, average='micro')\n",
    "print(\"Precision score of our model with Decision Tree :\", precision)\n",
    "recall = recall_score(y_true=tar_test, y_pred=predictionsDT, average='micro')\n",
    "print(\"Recall score of our model with Decision Tree :\", recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# for x in range(3):\n",
    "#     fpr[x], tpr[x], _ = roc_curve(tar_test[:], probDT[:, x], pos_label=x)\n",
    "#     roc_auc[x] = auc(fpr[x], tpr[x])\n",
    "#     print(\"AUC values of the decision tree\", roc_auc[x])\n",
    "#     plt.plot(fpr[x], tpr[x], color=colors[x], label='ROC curve (area = %0.2f)' % roc_auc[x])\n",
    "#\n",
    "# print(\"Accuracy score of our model with DTC :\", accuracy_score(tar_test, predictions))\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score of our model with MLP : 0.8662420382165605\n",
      "Accuracy score of our model with MLP under cross validation : 0.9131446509260049\n"
     ]
    }
   ],
   "source": [
    "#######################Multilayer Preceptron###################################\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "clf = MLPClassifier(learning_rate='constant',activation='logistic',solver='sgd',max_iter=4000,learning_rate_init=0.001)\n",
    "clf.fit(pred_train,np.ravel(tar_train,order='C'))\n",
    "predictionsMLP = clf.predict(pred_test)\n",
    "probMLP=clf.predict_proba(pred_test)\n",
    "print(\"Accuracy score of our model with MLP :\", accuracy_score(tar_test, predictionsMLP))\n",
    "scores = cross_val_score(clf, predictors, target, cv=10)\n",
    "print(\"Accuracy score of our model with MLP under cross validation :\", scores.mean())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix for Decision Tree:\n",
      "[[39  1  4  8]\n",
      " [ 0 14  0  0]\n",
      " [ 2  0 10  0]\n",
      " [ 8  8  1 62]]\n",
      "Confusion Matrix for MLP Classifier:\n",
      "[[43  0  5  1]\n",
      " [ 0 15  0  8]\n",
      " [ 3  0 12  0]\n",
      " [ 3  1  0 66]]\n"
     ]
    }
   ],
   "source": [
    "#Confusion Matrix -\n",
    "\n",
    "#Decision Tree\n",
    "print('Confusion matrix for Decision Tree:')\n",
    "cmDT=confusion_matrix(predictionsDT,tar_test)\n",
    "print(cmDT)\n",
    "\n",
    "#MLP Classifier\n",
    "print('Confusion Matrix for MLP Classifier:')\n",
    "cmMLP=confusion_matrix(tar_test,predictionsMLP)\n",
    "print(cmMLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.03225806 0.04032258 0.         0.92741935] [0.09319385 0.00513044 0.00550015 0.89617555]\n",
      "[0.03225806 0.04032258 0.         0.92741935] [0.026705   0.40988723 0.01612901 0.54727876]\n",
      "[0.03225806 0.04032258 0.         0.92741935] [0.05659767 0.00699311 0.00718347 0.92922576]\n",
      "[0.01694915 0.         0.98305085 0.        ] [0.00979226 0.00147394 0.98718635 0.00154744]\n",
      "[0.89189189 0.01801802 0.08108108 0.00900901] [0.95054714 0.00151389 0.03345238 0.01448659]\n",
      "[0.03225806 0.04032258 0.         0.92741935] [0.00424976 0.00857528 0.00170578 0.98546918]\n",
      "[0.03225806 0.04032258 0.         0.92741935] [0.00840756 0.01125046 0.00710372 0.97323826]\n",
      "[0.         0.96363636 0.         0.03636364] [0.01096021 0.9158615  0.01204177 0.06113651]\n",
      "[0.89189189 0.01801802 0.08108108 0.00900901] [0.96814421 0.00114184 0.0190931  0.01162085]\n",
      "[0.03225806 0.04032258 0.         0.92741935] [0.22181091 0.0255757  0.0246179  0.72799549]\n",
      "[0.89189189 0.01801802 0.08108108 0.00900901] [0.95665237 0.00510097 0.03220901 0.00603765]\n",
      "[0.89189189 0.01801802 0.08108108 0.00900901] [0.81177585 0.00211486 0.0678352  0.11827409]\n",
      "[0.89189189 0.01801802 0.08108108 0.00900901] [0.36858939 0.00828981 0.6008799  0.02224089]\n",
      "[0.89189189 0.01801802 0.08108108 0.00900901] [0.95383436 0.00110503 0.03299566 0.01206495]\n",
      "[0.03225806 0.04032258 0.         0.92741935] [0.05871063 0.00588189 0.00891505 0.92649243]\n",
      "[0.89189189 0.01801802 0.08108108 0.00900901] [0.93481325 0.01896699 0.0439357  0.00228406]\n",
      "[0.89189189 0.01801802 0.08108108 0.00900901] [0.98679062 0.00241917 0.00844621 0.002344  ]\n",
      "[0.03225806 0.04032258 0.         0.92741935] [0.03946745 0.00441774 0.00878019 0.94733462]\n",
      "[0.89189189 0.01801802 0.08108108 0.00900901] [0.97083224 0.00535925 0.02195996 0.00184855]\n",
      "[0.01694915 0.         0.98305085 0.        ] [0.00454527 0.00160308 0.99165174 0.00219991]\n",
      "[0.89189189 0.01801802 0.08108108 0.00900901] [0.97180979 0.00117268 0.0215226  0.00549492]\n",
      "[0.03225806 0.04032258 0.         0.92741935] [0.00998438 0.00788843 0.00181603 0.98031116]\n",
      "[0.01694915 0.         0.98305085 0.        ] [6.60648616e-03 1.64337958e-03 9.90846417e-01 9.03717018e-04]\n",
      "[0.03225806 0.04032258 0.         0.92741935] [0.02537493 0.056177   0.00427083 0.91417723]\n",
      "[0.85714286 0.         0.14285714 0.        ] [0.01046472 0.00590975 0.00374676 0.97987876]\n",
      "[0.         0.96363636 0.         0.03636364] [0.00850806 0.94664198 0.00767453 0.03717543]\n",
      "[0.89189189 0.01801802 0.08108108 0.00900901] [0.98763607 0.00167641 0.00715864 0.00352888]\n",
      "[0.01694915 0.         0.98305085 0.        ] [0.01378048 0.00368828 0.98149845 0.00103279]\n",
      "[0.         0.96363636 0.         0.03636364] [0.01009215 0.89434256 0.0072083  0.08835699]\n",
      "[0.03225806 0.04032258 0.         0.92741935] [0.0577545  0.01228844 0.00401802 0.92593904]\n",
      "[0.85714286 0.         0.14285714 0.        ] [0.93626296 0.00104428 0.03519396 0.0274988 ]\n",
      "[0.89189189 0.01801802 0.08108108 0.00900901] [0.03253691 0.83220725 0.01810844 0.1171474 ]\n",
      "[0.03225806 0.04032258 0.         0.92741935] [0.02266779 0.00380285 0.00522318 0.96830617]\n",
      "[0.89189189 0.01801802 0.08108108 0.00900901] [0.94423437 0.0017574  0.04671091 0.00729732]\n",
      "[0.89189189 0.01801802 0.08108108 0.00900901] [0.91768951 0.00131521 0.06772066 0.01327462]\n",
      "[0.03225806 0.04032258 0.         0.92741935] [0.03168222 0.00629576 0.00389167 0.95813035]\n",
      "[0.03225806 0.04032258 0.         0.92741935] [0.04830694 0.00464132 0.04174637 0.90530537]\n",
      "[0.03225806 0.04032258 0.         0.92741935] [0.01389334 0.01311361 0.01599787 0.95699517]\n",
      "[0.89189189 0.01801802 0.08108108 0.00900901] [0.77726923 0.00193368 0.20733006 0.01346704]\n",
      "[0.03225806 0.04032258 0.         0.92741935] [0.00588167 0.00603648 0.00508308 0.98299878]\n",
      "[0.03225806 0.04032258 0.         0.92741935] [0.468217   0.0041993  0.01034609 0.51723761]\n",
      "[0.03225806 0.04032258 0.         0.92741935] [0.04294173 0.00684803 0.00243167 0.94777858]\n",
      "[0.01694915 0.         0.98305085 0.        ] [0.11522346 0.01342083 0.87020548 0.00115023]\n",
      "[0.01694915 0.         0.98305085 0.        ] [6.45388843e-03 1.39141572e-03 9.91197652e-01 9.57044334e-04]\n",
      "[0.89189189 0.01801802 0.08108108 0.00900901] [0.98846243 0.00157405 0.00755805 0.00240547]\n",
      "[0.89189189 0.01801802 0.08108108 0.00900901] [0.97527125 0.00112363 0.015704   0.00790113]\n",
      "[0.85714286 0.         0.14285714 0.        ] [0.91314388 0.00159413 0.02092131 0.06434067]\n",
      "[0.89189189 0.01801802 0.08108108 0.00900901] [0.18103139 0.01520488 0.79846348 0.00530025]\n",
      "[0.         0.96363636 0.         0.03636364] [0.0095687  0.96136892 0.00533526 0.02372712]\n",
      "[0.85714286 0.         0.14285714 0.        ] [0.12464191 0.00575714 0.02253188 0.84706907]\n",
      "[0.89189189 0.01801802 0.08108108 0.00900901] [0.96887288 0.00139929 0.01514296 0.01458487]\n",
      "[0.03225806 0.04032258 0.         0.92741935] [0.01748731 0.00616702 0.00211006 0.97423561]\n",
      "[0.03225806 0.04032258 0.         0.92741935] [0.03239477 0.00659474 0.00313344 0.95787706]\n",
      "[0.85714286 0.         0.14285714 0.        ] [0.01948847 0.00618883 0.0078974  0.96642529]\n",
      "[0. 0. 0. 1.] [0.56393823 0.24812585 0.02456462 0.16337129]\n",
      "[0.         0.96363636 0.         0.03636364] [0.00890605 0.86360704 0.01030357 0.11718333]\n",
      "[0.03225806 0.04032258 0.         0.92741935] [0.01330645 0.00595622 0.00197264 0.97876469]\n",
      "[0.89189189 0.01801802 0.08108108 0.00900901] [0.99049482 0.00132148 0.00474915 0.00343455]\n",
      "[0.03225806 0.04032258 0.         0.92741935] [0.31398436 0.00433886 0.01151184 0.67016494]\n",
      "[0.03225806 0.04032258 0.         0.92741935] [0.01527421 0.05610449 0.02426323 0.90435807]\n",
      "[0.85714286 0.         0.14285714 0.        ] [0.06535035 0.00889106 0.00916436 0.91659423]\n",
      "[0.85714286 0.         0.14285714 0.        ] [0.0249186  0.00565238 0.0101592  0.95926982]\n",
      "[0.03225806 0.04032258 0.         0.92741935] [0.96956278 0.00290215 0.00791551 0.01961956]\n",
      "[0.89189189 0.01801802 0.08108108 0.00900901] [0.24163377 0.02534224 0.72846343 0.00456056]\n",
      "[0.03225806 0.04032258 0.         0.92741935] [0.00879804 0.0071788  0.00271728 0.98130588]\n",
      "[0.85714286 0.         0.14285714 0.        ] [0.04786727 0.00681719 0.01192942 0.93338612]\n",
      "[0.85714286 0.         0.14285714 0.        ] [0.90757877 0.00107471 0.01378975 0.07755678]\n",
      "[0.85714286 0.         0.14285714 0.        ] [0.94262043 0.0011911  0.01120185 0.04498662]\n",
      "[0.03225806 0.04032258 0.         0.92741935] [0.01610399 0.00682937 0.00235858 0.97470806]\n",
      "[0.01694915 0.         0.98305085 0.        ] [3.60199361e-02 4.88284886e-03 9.58247788e-01 8.49426757e-04]\n",
      "[0.89189189 0.01801802 0.08108108 0.00900901] [0.97986026 0.00415115 0.00914146 0.00684713]\n",
      "[0.89189189 0.01801802 0.08108108 0.00900901] [0.98389891 0.00133267 0.00903924 0.00572918]\n",
      "[0.01694915 0.         0.98305085 0.        ] [0.53671742 0.02372767 0.43722483 0.00233008]\n",
      "[0.01694915 0.         0.98305085 0.        ] [0.28805233 0.01487933 0.69479588 0.00227246]\n",
      "[0.89189189 0.01801802 0.08108108 0.00900901] [0.89823332 0.00133506 0.08178018 0.01865144]\n",
      "[0.03225806 0.04032258 0.         0.92741935] [0.00444106 0.04400783 0.01306286 0.93848825]\n",
      "[0.89189189 0.01801802 0.08108108 0.00900901] [0.62961381 0.00752863 0.3600517  0.00280586]\n",
      "[0. 0. 0. 1.] [0.72877181 0.00352948 0.03048142 0.23721728]\n",
      "[0.03225806 0.04032258 0.         0.92741935] [0.00762575 0.00787802 0.00158849 0.98290774]\n",
      "[0.89189189 0.01801802 0.08108108 0.00900901] [9.86745917e-01 9.62590607e-04 7.63314058e-03 4.65835144e-03]\n",
      "[0.03225806 0.04032258 0.         0.92741935] [0.0045144  0.00596134 0.00196026 0.987564  ]\n",
      "[0.85714286 0.         0.14285714 0.        ] [0.00710867 0.00708516 0.00321694 0.98258922]\n",
      "[0.03225806 0.04032258 0.         0.92741935] [0.03112599 0.04000751 0.00781026 0.92105625]\n",
      "[0.89189189 0.01801802 0.08108108 0.00900901] [0.95863622 0.00113538 0.02208506 0.01814335]\n",
      "[0.03225806 0.04032258 0.         0.92741935] [0.02019028 0.00839205 0.00159542 0.96982225]\n",
      "[0.89189189 0.01801802 0.08108108 0.00900901] [0.85118554 0.00845997 0.13525185 0.00510264]\n",
      "[0. 0. 0. 1.] [0.96894044 0.00175519 0.01402225 0.01528211]\n",
      "[0.89189189 0.01801802 0.08108108 0.00900901] [0.976746   0.00269508 0.01830702 0.0022519 ]\n",
      "[0.03225806 0.04032258 0.         0.92741935] [0.02043254 0.00562038 0.00394186 0.97000522]\n",
      "[0.85714286 0.         0.14285714 0.        ] [0.81199176 0.00116883 0.03092763 0.15591178]\n",
      "[0.03225806 0.04032258 0.         0.92741935] [0.0245843  0.03157051 0.01079852 0.93304666]\n",
      "[0.03225806 0.04032258 0.         0.92741935] [0.26913447 0.01454965 0.00564759 0.71066829]\n",
      "[0.03225806 0.04032258 0.         0.92741935] [0.98447805 0.00167582 0.00684492 0.00700122]\n",
      "[0.89189189 0.01801802 0.08108108 0.00900901] [0.98283031 0.00164673 0.01332474 0.00219822]\n",
      "[0.03225806 0.04032258 0.         0.92741935] [0.00898559 0.00616796 0.00207412 0.98277233]\n",
      "[0.03225806 0.04032258 0.         0.92741935] [0.01561221 0.04838819 0.0039158  0.9320838 ]\n",
      "[0.03225806 0.04032258 0.         0.92741935] [0.23416704 0.02791704 0.00542869 0.73248723]\n",
      "[0.03225806 0.04032258 0.         0.92741935] [0.01981013 0.01574249 0.00665269 0.95779469]\n",
      "[0.03225806 0.04032258 0.         0.92741935] [0.41290182 0.00366675 0.01046709 0.57296433]\n",
      "[0.03225806 0.04032258 0.         0.92741935] [0.02158406 0.00533162 0.0087585  0.96432583]\n",
      "[0.01694915 0.         0.98305085 0.        ] [0.04815291 0.00135482 0.94169229 0.00879997]\n",
      "[0.03225806 0.04032258 0.         0.92741935] [0.10119982 0.01162451 0.00997572 0.87719996]\n",
      "[0.89189189 0.01801802 0.08108108 0.00900901] [0.22712552 0.00607511 0.76127135 0.00552803]\n",
      "[0.03225806 0.04032258 0.         0.92741935] [0.01888791 0.00649775 0.00699333 0.96762102]\n",
      "[0.03225806 0.04032258 0.         0.92741935] [0.02880009 0.00653598 0.00251164 0.96215229]\n",
      "[0.03225806 0.04032258 0.         0.92741935] [0.00373387 0.06212532 0.01384234 0.92029847]\n",
      "[0.03225806 0.04032258 0.         0.92741935] [0.04172752 0.0286325  0.00948214 0.92015785]\n",
      "[0.03225806 0.04032258 0.         0.92741935] [0.41816048 0.02084886 0.01801526 0.54297541]\n",
      "[0.03225806 0.04032258 0.         0.92741935] [0.35867912 0.03207805 0.0103256  0.59891724]\n",
      "[0.89189189 0.01801802 0.08108108 0.00900901] [0.95953915 0.00343491 0.03443433 0.00259161]\n",
      "[0.89189189 0.01801802 0.08108108 0.00900901] [0.97842684 0.00138508 0.01609838 0.00408971]\n",
      "[0.01694915 0.         0.98305085 0.        ] [2.32732679e-02 8.06532835e-04 9.71036659e-01 4.88354061e-03]\n",
      "[0.89189189 0.01801802 0.08108108 0.00900901] [0.98340613 0.00304246 0.0105269  0.00302452]\n",
      "[0.89189189 0.01801802 0.08108108 0.00900901] [0.18293296 0.00181743 0.79650069 0.01874892]\n",
      "[0.         0.96363636 0.         0.03636364] [0.00971553 0.94715778 0.00682435 0.03630235]\n",
      "[0.89189189 0.01801802 0.08108108 0.00900901] [0.98855816 0.00158188 0.00787256 0.00198741]\n",
      "[0.         0.96363636 0.         0.03636364] [0.01124344 0.9569363  0.00519095 0.0266293 ]\n",
      "[0.89189189 0.01801802 0.08108108 0.00900901] [0.94817706 0.00116789 0.02914133 0.02151371]\n",
      "[0.03225806 0.04032258 0.         0.92741935] [0.01391339 0.31759222 0.01820977 0.65028461]\n",
      "[0.         0.96363636 0.         0.03636364] [0.01308705 0.94707225 0.00502492 0.03481579]\n",
      "[0.03225806 0.04032258 0.         0.92741935] [0.81495541 0.00093612 0.06385327 0.1202552 ]\n",
      "[0.03225806 0.04032258 0.         0.92741935] [0.01941314 0.89252877 0.00808794 0.07997015]\n",
      "[0.  0.5 0.  0.5] [0.01000396 0.33725773 0.01374885 0.63898947]\n",
      "[0.89189189 0.01801802 0.08108108 0.00900901] [0.98638427 0.0014647  0.01015305 0.00199797]\n",
      "[0.03225806 0.04032258 0.         0.92741935] [0.01893266 0.00484362 0.01192955 0.96429417]\n",
      "[0.03225806 0.04032258 0.         0.92741935] [0.00826137 0.01032931 0.01257356 0.96883577]\n",
      "[0.         0.96363636 0.         0.03636364] [0.01048951 0.94425777 0.00589621 0.0393565 ]\n",
      "[0. 0. 0. 1.] [0.05821211 0.02587451 0.02125363 0.89465974]\n",
      "[0.03225806 0.04032258 0.         0.92741935] [0.01503622 0.07503004 0.00398569 0.90594805]\n",
      "[0.03225806 0.04032258 0.         0.92741935] [0.009006   0.00614953 0.00272212 0.98212234]\n",
      "[0.03225806 0.04032258 0.         0.92741935] [0.00524571 0.00750719 0.00192945 0.98531765]\n",
      "[0. 0. 0. 1.] [0.87258235 0.0043269  0.10175606 0.02133469]\n",
      "[0. 0. 0. 1.] [0.05210796 0.03696026 0.03771263 0.87321915]\n",
      "[0.03225806 0.04032258 0.         0.92741935] [0.08389901 0.0046659  0.00695797 0.90447713]\n",
      "[0.03225806 0.04032258 0.         0.92741935] [0.01594376 0.88676152 0.00762055 0.08967416]\n",
      "[0.         0.96363636 0.         0.03636364] [0.02410575 0.81281235 0.00794618 0.15513571]\n",
      "[0.03225806 0.04032258 0.         0.92741935] [0.97725126 0.00158846 0.00715147 0.01400881]\n",
      "[0.03225806 0.04032258 0.         0.92741935] [0.04497332 0.00684331 0.00253333 0.94565004]\n",
      "[0.89189189 0.01801802 0.08108108 0.00900901] [0.01348661 0.00149407 0.9833684  0.00165093]\n",
      "[0.03225806 0.04032258 0.         0.92741935] [0.17760661 0.02996496 0.00496181 0.78746662]\n",
      "[0.03225806 0.04032258 0.         0.92741935] [0.01204615 0.18066952 0.01874679 0.78853753]\n",
      "[0.         0.96363636 0.         0.03636364] [0.00921038 0.95308934 0.00552455 0.03217573]\n",
      "[0.03225806 0.04032258 0.         0.92741935] [0.87997222 0.00502997 0.0075879  0.10740991]\n",
      "[0.03225806 0.04032258 0.         0.92741935] [0.00746881 0.00788606 0.00281572 0.98182941]\n",
      "[0.03225806 0.04032258 0.         0.92741935] [0.00571216 0.00952815 0.00191969 0.98284   ]\n",
      "[0.03225806 0.04032258 0.         0.92741935] [0.01056553 0.20109572 0.01786891 0.77046983]\n",
      "[0.03225806 0.04032258 0.         0.92741935] [0.10446144 0.01627163 0.00461739 0.87464954]\n",
      "[0.         0.96363636 0.         0.03636364] [0.01144062 0.95232642 0.0047316  0.03150137]\n",
      "[0.03225806 0.04032258 0.         0.92741935] [0.0618571  0.01404464 0.00997945 0.91411881]\n",
      "[0.03225806 0.04032258 0.         0.92741935] [0.01958481 0.90640857 0.00678042 0.0672262 ]\n",
      "[0.03225806 0.04032258 0.         0.92741935] [0.00688078 0.00760903 0.00594999 0.9795602 ]\n",
      "[0.89189189 0.01801802 0.08108108 0.00900901] [0.98574727 0.00112555 0.00949041 0.00363678]\n",
      "[0.01694915 0.         0.98305085 0.        ] [0.08338215 0.00157166 0.90539658 0.00964961]\n",
      "[0. 0. 0. 1.] [0.71929829 0.05601814 0.1713419  0.05334168]\n",
      "[0.03225806 0.04032258 0.         0.92741935] [0.0149589  0.01010492 0.00836076 0.96657542]\n",
      "[0.03225806 0.04032258 0.         0.92741935] [0.00357308 0.00574618 0.00464074 0.98604   ]\n",
      "[0.  0.5 0.  0.5] [0.0096873  0.26258801 0.00929001 0.71843468]\n"
     ]
    }
   ],
   "source": [
    "#######################Q2#############################################\n",
    "\n",
    "from tabulate import tabulate\n",
    "# count_d=0\n",
    "# count_h=0\n",
    "# print(class_target)\n",
    "# for x in range(class_target.len()):\n",
    "for i in range(len(pred_test)):\n",
    " print(probDT[i],probMLP[i])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class      Decision Tree Classifier    MLP Classifier\n",
      "-------  --------------------------  ----------------\n",
      "d                         0.0322581        0.0931939\n",
      "h                         0.0403226        0.00513044\n",
      "o                         0                0.00550015\n",
      "s                         0.927419         0.896176\n"
     ]
    }
   ],
   "source": [
    "proba1=[]\n",
    "s1 = ['d','h','o','s']\n",
    "for i in range(4):\n",
    " prob2=[s1[i],probDT[0][i],probMLP[0][i]]\n",
    " \n",
    " proba1.append(prob2)\n",
    "\n",
    "print(tabulate(proba1, headers=['Class', 'Decision Tree Classifier','MLP Classifier']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 3, 3, 2, 0, 3, 3, 1, 0, 3, 0, 0, 0, 0, 3, 0, 0, 3, 0, 2, 0, 3, 2, 3, 3, 1, 0, 2, 1, 3, 0, 0, 3, 0, 0, 3, 3, 3, 0, 3, 3, 3, 2, 2, 0, 0, 0, 0, 1, 0, 0, 3, 3, 3, 3, 1, 3, 0, 3, 3, 0, 3, 0, 0, 3, 3, 0, 0, 3, 2, 0, 0, 2, 2, 0, 3, 0, 3, 3, 0, 3, 3, 3, 0, 3, 0, 0, 0, 3, 0, 3, 3, 0, 0, 3, 3, 3, 3, 3, 3, 2, 3, 0, 3, 3, 3, 3, 3, 3, 0, 0, 2, 0, 0, 1, 0, 1, 0, 3, 1, 3, 3, 3, 0, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 1, 0, 3, 2, 3, 3, 1, 3, 3, 3, 3, 3, 1, 3, 3, 3, 0, 2, 3, 3, 3, 3]\n",
      "The accuracy of the Average Aggregate Classifier is: 0.821656050955414\n"
     ]
    }
   ],
   "source": [
    "########################04###############################################3\n",
    "\n",
    "\n",
    "\n",
    "def avgClassifier(pred_train,tar_train,pred_test):\n",
    "\n",
    " classifier = DecisionTreeClassifier(max_depth=3)  # configure the classifier\n",
    " classifier = classifier.fit(pred_train, tar_train)  # train a decision tree model\n",
    " # predictions = classifier.predict(pred_test)  # deploy model and make predictions on test set\n",
    " probDT = classifier.predict_proba(pred_test)\n",
    "\n",
    " clf = MLPClassifier(learning_rate='constant', activation='logistic', solver='sgd', max_iter=4000,\n",
    "                     learning_rate_init=0.001)\n",
    " clf.fit(pred_train, np.ravel(tar_train, order='C'))\n",
    " # predictions1 = clf.predict(pred_test)\n",
    " probMLP = clf.predict_proba(pred_test)\n",
    " avg_predictions=[]\n",
    " avg_prob = (probDT + probMLP) / 2\n",
    " max_avg_pred=0\n",
    " for i in range(len(pred_test)):\n",
    "  l=[avg_prob[i][0],avg_prob[i][1],avg_prob[i][2],avg_prob[i][3]]\n",
    "  max_avg_pred=l.index(max(l))\n",
    "  avg_predictions.append(max_avg_pred)\n",
    "  # print(max_avg_pred,avg_prob[i])\n",
    " return avg_predictions\n",
    "\n",
    "avg_predictions=avgClassifier(pred_train,tar_train,pred_test)\n",
    "print(avg_predictions)\n",
    "print('The accuracy of the Average Aggregate Classifier is:',accuracy_score(tar_test,avg_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 3, 3, 2, 0, 3, 3, 1, 0, 3, 0, 0, 0, 0, 3, 0, 0, 3, 0, 2, 0, 3, 2, 3, 3, 1, 0, 2, 1, 3, 0, 1, 3, 0, 0, 3, 3, 3, 0, 3, 3, 3, 2, 2, 0, 0, 0, 0, 1, 3, 0, 3, 3, 3, 3, 1, 3, 0, 3, 3, 3, 3, 0, 0, 3, 3, 0, 0, 3, 2, 0, 0, 2, 2, 0, 3, 0, 3, 3, 0, 3, 3, 3, 0, 3, 0, 0, 0, 3, 0, 3, 3, 0, 0, 3, 3, 3, 3, 3, 3, 2, 3, 0, 3, 3, 3, 3, 3, 3, 0, 0, 2, 0, 0, 1, 0, 1, 0, 3, 1, 3, 1, 3, 0, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 1, 1, 0, 3, 0, 3, 3, 1, 3, 3, 3, 3, 3, 1, 3, 1, 3, 0, 2, 3, 3, 3, 3]\n",
      "The accuracy of the Conditional Classifier is: 0.8535031847133758\n"
     ]
    }
   ],
   "source": [
    "##############################Q5#####################\n",
    "\n",
    "\n",
    "def conditionalClassifier(pred_train,tar_train,pred_test,tar_test):\n",
    " ##############p(a/b)=p(a&b)/p(b) then p(class=s|Dt=s)=p(class=s&DT=s)/p(DT=s)\n",
    "\n",
    " ###############DT#######\n",
    " classifier = DecisionTreeClassifier(max_depth=3)  # configure the classifier\n",
    " classifier = classifier.fit(pred_train, tar_train)  # train a decision tree model\n",
    " predictionsDT = classifier.predict(pred_test)  # deploy model and make predictions on test set\n",
    " probDT = classifier.predict_proba(pred_test)\n",
    "\n",
    " cmDT = confusion_matrix(tar_test,predictionsDT)\n",
    " condProbDT=[]\n",
    " for i in range(4):\n",
    "  sumDT = 0\n",
    "  for j in range(4):\n",
    "   sumDT += cmDT[j][i]\n",
    "  condProbDT.append(cmDT[i][i]/sumDT)\n",
    "\n",
    "\n",
    " #########################MLP########\n",
    " clf = MLPClassifier(learning_rate='constant', activation='logistic', solver='sgd', max_iter=4000,\n",
    "                     learning_rate_init=0.001)\n",
    " clf.fit(pred_train, np.ravel(tar_train, order='C'))\n",
    " predictionsMLP = clf.predict(pred_test)\n",
    " probMLP = clf.predict_proba(pred_test)\n",
    " cmMLP = confusion_matrix(tar_test, predictionsMLP)\n",
    " # print(cmMLP)\n",
    " condProbMLP=[]\n",
    " for i in range(4):\n",
    "  sumMLP=0\n",
    "  for j in range(4):\n",
    "   sumMLP+=cmMLP[j][i]\n",
    "  condProbMLP.append(cmMLP[i][i]/sumMLP)\n",
    "  # l=cmMLP[:][i]\n",
    "  # print(cmMLP[:][i],cmMLP[i][:])\n",
    "  # print(\"abcasdasdsa\",sum(cmMLP[:][i]),sumMLP,sum(l))\n",
    " condProb=[]\n",
    " #########################Conditional Probability classifier#############\n",
    " for i in range(len(predictionsDT)):\n",
    "  if(probDT[i][predictionsDT[i]]*condProbDT[predictionsDT[i]]>probMLP[i][predictionsMLP[i]]*condProbMLP[predictionsMLP[i]]):\n",
    "   condProb.append(predictionsDT[i])\n",
    "  else:\n",
    "   condProb.append(predictionsMLP[i])\n",
    "\n",
    " return condProb\n",
    "\n",
    "conditional_Predicitions=conditionalClassifier(pred_train,tar_train,pred_test,tar_test)\n",
    "print(conditional_Predicitions)\n",
    "print('The accuracy of the Conditional Classifier is:',accuracy_score(tar_test,conditional_Predicitions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
