{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------BLOG TEXT MINING ---------#\n",
    "\n",
    "# TASK DESC - A large data set of blogs having 19,300 xml files is processed. Each XML file contains blog of different\n",
    "# Demographic category. The xml files are named as per the demography.\n",
    "\n",
    "# The objective is to find the most dominant topic in the blog in order to find whats the discussion going on in\n",
    "# a particular demograpyh group. This contains three seperate files(Mentioned below) which implement different topic\n",
    "# Modelling models and performs a comparitive study of their performances.\n",
    "\n",
    "# Blog_text_mine - Deals with Data cleaning, XML Parsing, Tokenization, Lemmatization and creating a Dictionary from\n",
    "#                   cleaned data and saving them in pickle files.\n",
    "\n",
    "# LDA_Implement - Implementing LDA model to find dominant topic\n",
    "# LSA_Implement - Implementing LSA model to find dominant topic\n",
    "# TF-IDF_Implement - Implementing LSA+TF-IDF and LDA +TF-IDF to find dominant topics\n",
    "\n",
    "import  xml.etree.ElementTree as ET\n",
    "import gensim\n",
    "import nltk\n",
    "from nltk import word_tokenize, pos_tag, WordNetLemmatizer\n",
    "from sphinx.util import stemmer\n",
    "import time\n",
    "from lxml import etree\n",
    "import xml.etree.cElementTree as etree\n",
    "\n",
    "\n",
    "\n",
    "#** XML FILE PARSING IS DONE USING ELEMENT TREE ***#\n",
    "\n",
    "def doXMLParsing(file):\n",
    "    xmlstring = r'D:\\college\\sem2\\Text Mining\\Assignment_Blog_topic\\Assignment2BlogData\\blogs'\n",
    "    joinstring = '/'\n",
    "    # count += 1\n",
    "    contents = []\n",
    "\n",
    "    with open(xmlstring + joinstring + file, 'r', encoding=\"ANSI\") as f:\n",
    "        contents = f.read()\n",
    "        tree = ET.fromstring(contents)\n",
    "    f.close()\n",
    "    parser = etree.XMLParser(recover=True)\n",
    "    treestring=etree.fromstring(xmlstring, parser=parser)\n",
    "    tree = etree.XML(treestring, parser)\n",
    "    return tree\n",
    "\n",
    "\n",
    "#** STOP SYMBOLS REMOVED BEFORE TOKENIZATION **#\n",
    "\n",
    "\n",
    "\n",
    "def remove_stop_symols(word):\n",
    "\n",
    "    a12=word.replace(\"blog\",\" \")\n",
    "    a13=a12.replace(\"date\", \" \")\n",
    "    a14=a13.replace(\"post\",\" \")\n",
    "    a16=a14.replace(\"like\",\" \")\n",
    "    a17 = a16.replace(\"know\", \" \")\n",
    "    a18 = a17.replace(\"time\", \" \")\n",
    "    a19 = a18.replace(\"think\", \" \")\n",
    "    a20 = a19.replace(\"thing\", \" \")\n",
    "    a21= a20.replace(\"urllink\",\" \")\n",
    "    a22=a21.replace(\"nbsp\",\" \")\n",
    "    a23=a22.replace(\"'\",\" \")\n",
    "    stop_symbols=\".,-!?\\\"()<>/*\\\\\".split()\n",
    "    stop_word=['1','2','3','4','5','6','7','8','9','0','=']\n",
    "\n",
    "    a15=\"\"\n",
    "    for i in a22:\n",
    "        if(i not in stop_word and i not in stop_symbols):\n",
    "\n",
    "            a15+=i\n",
    "    return a15\n",
    "\n",
    "start=time.time()\n",
    "from lxml import etree\n",
    "import io\n",
    "mypath=r'D:\\college\\sem2\\Text Mining\\Assignment_Blog_topic\\Assignment2BlogData\\blogs'\n",
    "from os import listdir\n",
    "from os.path import isfile,join\n",
    "onlyfiles=[f for f in listdir(mypath) if isfile(join(mypath,f))]\n",
    "female_no=[]\n",
    "male_no=[]\n",
    "al_20=[]\n",
    "au_20=[]\n",
    "all_no=[]\n",
    "\n",
    "\n",
    "#** READING FILES INTO SEPERATE LISTS BASED ON DEMOGRAPHY **#\n",
    "\n",
    "i=0\n",
    "\n",
    "for file in onlyfiles:\n",
    " fileinfo=file.split('.')\n",
    " if(fileinfo[1]==\"female\"):\n",
    "   female_no.append(file)\n",
    " elif(fileinfo[1]==\"male\"):\n",
    "    male_no.append(file)\n",
    " if(int(fileinfo[2])<=20):\n",
    "    al_20.append(file)\n",
    " elif(int(fileinfo[2])>20):\n",
    "    au_20.append(file)\n",
    " all_no.append(file)\n",
    "\n",
    "\n",
    "#** FUNCTION TO SELECT DATASET BASED ON DEMOGRAPHY **#\n",
    "\n",
    "def selectDemographyData(demtype):\n",
    "    print(demtype)\n",
    "    if(demtype==\"male\"):\n",
    "        return male_no\n",
    "        print(\"male\")\n",
    "    elif(demtype==\"female\"):\n",
    "        return female_no\n",
    "        print(\"female is found\")\n",
    "    elif(demtype==\"under20\"):\n",
    "        return al_20\n",
    "    elif(demtype==\"above20\"):\n",
    "        return au_20\n",
    "\n",
    "    else:\n",
    "        return all_no\n",
    "\n",
    "def readFileData(demotype):\n",
    " i=0\n",
    " data_al20 = []\n",
    "\n",
    "\n",
    " for file in au_20:\n",
    "  xmlstring=r'D:\\college\\sem2\\Text Mining\\Assignment_Blog_topic\\Assignment2BlogData\\blogs'\n",
    "  joinstring='/'\n",
    "\n",
    "  a=[]\n",
    "  with open(xmlstring+joinstring+file ,'r',encoding=\"ANSI\") as f:\n",
    "   a=f.read()\n",
    "  data_al20.append(a.lower())\n",
    "  f.close()\n",
    "  i+=1\n",
    "  if(i==20):\n",
    "     break\n",
    " return data_al20\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#**  FUNCTION FOR FINDING TWO NOUNS/VERB BEFORE AND AFTER CLUSTER **#\n",
    "\n",
    "def findNounAndVerb(textFile,wordlist):\n",
    " texttagged=nltk.pos_tag(nltk.word_tokenize(textFile))\n",
    " for i,(word,pos) in enumerate(texttagged):\n",
    "  deslist = ['NN', 'NNS', 'NNP', 'NNPS', 'VBD', 'VBN', 'VBP', 'VBZ']\n",
    "  if(word == wordlist):\n",
    "    print('Nouns/Verb before and After the word:',word)\n",
    "    nvBefore=[]\n",
    "    nvAfter=[]\n",
    "    for j in range(i-1,0,-1):\n",
    "     # print(texttagged)\n",
    "     if(texttagged[j][1] in deslist):\n",
    "      nvBefore.append(texttagged[j][0])\n",
    "      if(len(nvBefore)==2):\n",
    "       break\n",
    "    print ('Cluster Before:',nvBefore)\n",
    "    for j in range(i+1,len(texttagged)):\n",
    "     if (texttagged[j][1] in deslist):\n",
    "      nvAfter.append(texttagged[j][0])\n",
    "      if (len(nvAfter) == 2):\n",
    "       break\n",
    "    print('Cluster After:',nvAfter)\n",
    "\n",
    "\n",
    "\n",
    "######Data Pre-processing #############\n",
    "#LEMMATIZATION, TOKENIZATION AND POS TAGGING\n",
    "\n",
    "\n",
    "def lemmatize_stemming(text):\n",
    "    wnl=WordNetLemmatizer()\n",
    "    return wnl.lemmatize(word=text)\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    garbage_word = ['july', 'august','people','today','june','year','week','night','january','feburary','march','april'\n",
    "        ,'may','september','october','november','december','anyways','haha','shit','fuck']\n",
    "\n",
    "    deslist = ['NN', 'NNS', 'NNP', 'NNPS']\n",
    "    for token,des in nltk.pos_tag(nltk.word_tokenize(text)):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "            if des in deslist and token not in garbage_word:\n",
    "             result.append(lemmatize_stemming(token))\n",
    "\n",
    "    return result\n",
    "\n",
    "print(\"hello\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def processTheData(textFile):\n",
    " pre_al20 = []\n",
    " for xyz in readFileData(textFile):\n",
    "  pre_al20.append(remove_stop_symols(xyz))\n",
    " print(\"hello\")\n",
    " pre_al21=[]\n",
    " for xyz in pre_al20:\n",
    "  pre_al21.append(preprocess(xyz))\n",
    " return pre_al21\n",
    "\n",
    "from gensim import corpora\n",
    "import pickle\n",
    "print(\"hello\")\n",
    "\n",
    "\n",
    "\n",
    "#** FUNCTION FOR CREATING CORPUS AND DICTIONARY AND SAVING THEM AS PICKLE FILES **##\n",
    "def createCorpusAndDictionary(textFile,demograpy=\"all\"):\n",
    " finalData=processTheData(textFile)\n",
    " dictionary = corpora.Dictionary(finalData)\n",
    " corpus = [dictionary.doc2bow(text) for text in finalData]\n",
    " pickle.dump(corpus, open('corpus'+demograpy+'.pkl', 'wb'))\n",
    " dictionary.save('dictionary'+demograpy+'.gensim')\n",
    "\n",
    "print(\"hello\")\n",
    "\n",
    "\n",
    "def centreControl(demography):\n",
    "    createCorpusAndDictionary(readFileData(demography),demography)\n",
    "\n",
    "# Pass the demography for which you want to run and find the most dominant topic. In this case, most dominant topics of\n",
    "# female blogs have been found\n",
    "# Other Demographies are - Male, Above 20, Below 20 and All.\n",
    "centreControl(\"female\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster NUmber: 0\n",
      "friend\n",
      "life\n",
      "home\n",
      "school\n",
      "house\n",
      "Cluster NUmber: 1\n",
      "life\n",
      "friend\n",
      "work\n",
      "home\n",
      "school\n"
     ]
    }
   ],
   "source": [
    "# Blog_Text_Mine - Continuation\n",
    "\n",
    "# **** IMPLEMENTING LDA - Latent Dirichlet Allocation *********#\n",
    "\n",
    "# This Python code is extension of the Blog_Text_mine program task. In this code, we use the dictionary created in\n",
    "# file - Blog_text_mine file and then implement LDA model on the dictionary to find most\n",
    "# dominant topics in each demography.\n",
    "\n",
    "# Gensim - Latent Dirchlet Allocation is taken from this\n",
    "# Pickling\n",
    "\n",
    "#** IMPORTING REQUIRED LIBRARIES **#\n",
    "import gensim\n",
    "import pickle\n",
    "from gensim import corpora\n",
    "\n",
    "#** FUNCTION TO RUN LDA **#\n",
    "# -- LDA MODEL FITTING, CLUSTERING OF DOMINANT TOPICS\n",
    "\n",
    "def runLDA(n_topic=2,demographicType=\"all\"):\n",
    " num_topics=n_topic\n",
    " corpus=pickle.load(open('D:\\college\\sem2\\Text Mining\\Labs\\Lab_skip_gram\\corpus'+demographicType+'.pkl','rb'))\n",
    " ldamodel=gensim.models.ldamodel.LdaModel(corpus,num_topics=num_topics)\n",
    " ldamodel.save('model'+demographicType+'.gensim')\n",
    " dictionary=corpora.Dictionary.load('D:\\college\\sem2\\Text Mining\\Labs\\Lab_skip_gram\\dictionary'+demographicType+'.gensim')\n",
    " topics = ldamodel.print_topics(num_words=5)\n",
    " tops=[]\n",
    " for n, topic in enumerate(topics):\n",
    "    print(\"Cluster NUmber:\",n)\n",
    "    #print(topic[1][1])\n",
    "    for i,val in enumerate(topic[1]):\n",
    "        # print(topic)\n",
    "\n",
    "        if(val=='*'):\n",
    "            top=int(topic[1][i+2])\n",
    "            # print(top)\n",
    "            for k in range(3,10):\n",
    "                # print(topic[1][i+k])\n",
    "                if(topic[1][i+k].isdigit()):\n",
    "                 top=top*10+int(topic[1][i+k])\n",
    "                 # print(top)\n",
    "                else: break\n",
    "            # print(top)\n",
    "            print(dictionary[top])\n",
    " maxTerm=1\n",
    " num=0\n",
    " for xyz in corpus:\n",
    "   for abc in xyz:\n",
    "    if(maxTerm<abc[1]):\n",
    "        num=abc[0]\n",
    "        maxTerm=abc[1]\n",
    "\n",
    "# Pass the demography for which you want to run and find the most dominant topic. In this case, most dominant topics of\n",
    "# female blogs have been found\n",
    "# Other Demographies are - Male, Above 20, Below 20 and All.        \n",
    "runLDA(2,\"female\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster NUmber: 0\n",
      "jumper\n",
      "coolgal\n",
      "friend\n",
      "rosepedal\n",
      "life\n",
      "Cluster NUmber: 1\n",
      "jumper\n",
      "life\n",
      "friend\n",
      "home\n",
      "work\n"
     ]
    }
   ],
   "source": [
    "# Blog_Text_Mine - Continuation\n",
    "\n",
    "# **** IMPLEMENTING LSA - Latent Semantic Analysis *********#\n",
    "\n",
    "# This Python code is extension of the Blog_Text_mine program task. In this code, we use the dictionary created in\n",
    "# file - Blog_text_mine file and then implement LSA model on the dictionary to find most\n",
    "# dominant topics in each demography.\n",
    "\n",
    "# Gensim - Latent Semantic Analysis is taken from this\n",
    "# Pickling\n",
    "\n",
    "\n",
    "#** IMPORTING REQUIRED LIBRARIES **#\n",
    "import gensim\n",
    "import pickle\n",
    "from gensim import corpora\n",
    "from gensim import corpora, models\n",
    "\n",
    "\n",
    "#** FUNCTION TO IMPLEMENT LSA\n",
    "#-- LSA MODEL FITTING, CLUSTERING OF DOMINANT TOPICS\n",
    "def runLSA(n_topic=2,demographicType=\"all\"):\n",
    "    num_topics = n_topic\n",
    "    corpus = pickle.load(open('D:\\college\\sem2\\Text Mining\\Labs\\Lab_skip_gram\\corpus' + demographicType + '.pkl', 'rb'))\n",
    "    lsi = models.LsiModel(corpus, num_topics=num_topics)\n",
    "    lsi.save('model' + demographicType + '.gensim')\n",
    "    dictionary = corpora.Dictionary.load('D:\\college\\sem2\\Text Mining\\Labs\\Lab_skip_gram\\dictionary' + demographicType + '.gensim')\n",
    "    topics = lsi.print_topics(num_words=5)\n",
    "    tops = []\n",
    "    for n, topic in enumerate(topics):\n",
    "        print(\"Cluster NUmber:\", n)\n",
    "        # print(topic[1][1])\n",
    "        for i, val in enumerate(topic[1]):\n",
    "            # print(topic)\n",
    "\n",
    "            if (val == '*'):\n",
    "                top = int(topic[1][i + 2])\n",
    "                # print(top)\n",
    "                for k in range(3, 10):\n",
    "                    # print(topic[1][i+k])\n",
    "                    if (topic[1][i + k].isdigit()):\n",
    "                        top = top * 10 + int(topic[1][i + k])\n",
    "                        # print(top)\n",
    "                    else:\n",
    "                        break\n",
    "                # print(top)\n",
    "                print(dictionary[top])\n",
    "    maxTerm = 1\n",
    "    num = 0\n",
    "    for xyz in corpus:\n",
    "        for abc in xyz:\n",
    "            if (maxTerm < abc[1]):\n",
    "                num = abc[0]\n",
    "                maxTerm = abc[1]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Pass the demography for which you want to run and find the most dominant topic. In this case, most dominant topics of\n",
    "# female blogs have been found\n",
    "# Other Demographies are - Male, Above 20, Below 20 and All.\n",
    "runLSA(2,\"female\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster NUmber: 0\n",
      "weekend\n",
      "school\n",
      "love\n",
      "book\n",
      "house\n",
      "Cluster NUmber: 1\n",
      "movie\n",
      "friend\n",
      "love\n",
      "work\n",
      "school\n",
      "Maximum Term Frequency\n",
      "life 71011\n",
      "friend 65873\n",
      "Maximum TF-IDF\n",
      "school 65.51040229400336\n",
      "movie 61.7928721671062\n",
      "Cluster NUmber: 0\n",
      "movie\n",
      "school\n",
      "woman\n",
      "work\n",
      "weekend\n",
      "Cluster NUmber: 1\n",
      "dont\n",
      "thats\n",
      "bush\n",
      "hehe\n",
      "cause\n"
     ]
    }
   ],
   "source": [
    "# Blog_Text_Mine - Continuation\n",
    "\n",
    "# **** IMPLEMENTING TF-IDF COMBINED WITH LSA AND LDA *********#\n",
    "\n",
    "# This Python code is extension of the Blog_Text_mine program task. In this code, we use the dictionary created in\n",
    "# file - Blog_text_mine file and then implement TF-IDF+LDA and TF-IDF + LSA models on the dictionary to find most\n",
    "# dominant topics\n",
    "\n",
    "\n",
    "#** IMPORTING REQUIRED LIBRARIES **#\n",
    "import gensim\n",
    "import pickle\n",
    "from gensim import corpora\n",
    "from gensim import corpora, models\n",
    "\n",
    "#** FUNCTION IMPLEMENTING LDA AND LSA WITH TF-IDF\n",
    "\n",
    "def FindMaxValue(corpus,demographicType):\n",
    " maxvalue={}\n",
    " dictionary = corpora.Dictionary.load('D:\\college\\sem2\\Text Mining\\Labs\\Lab_skip_gram\\dictionary' + demographicType+ '.gensim')\n",
    " for doc in corpus:\n",
    "  for words,freq in doc:\n",
    "   if(words in maxvalue):\n",
    "    maxvalue[words]+=freq\n",
    "   else:\n",
    "    maxvalue[words]=freq\n",
    " newdict={k: v for k, v in sorted(maxvalue.items(), key=lambda item: item[1],reverse=True)}\n",
    " i=0\n",
    " for word in newdict:\n",
    "  i+=1\n",
    "  if(i==3):\n",
    "   break\n",
    "\n",
    "  print(dictionary[word],newdict[word])\n",
    "\n",
    "#** TF-IDF AND LDA\n",
    "def runtfidf(n_top=2,demographicType=\"all\"):\n",
    " corpus = pickle.load(open('D:\\college\\sem2\\Text Mining\\Labs\\Lab_skip_gram\\corpus' + demographicType + '.pkl', 'rb'))\n",
    " dictionary=corpora.Dictionary.load('D:\\college\\sem2\\Text Mining\\Labs\\Lab_skip_gram\\dictionary'+demographicType+'.gensim')\n",
    " tfidf = models.TfidfModel(corpus)\n",
    " corpus_tfidf = tfidf[corpus]\n",
    " ldamodel = gensim.models.ldamodel.LdaModel(corpus_tfidf, num_topics=2)\n",
    " ldamodel.save('modeltf' + demographicType + '.gensim')\n",
    " dictionary = corpora.Dictionary.load('D:\\college\\sem2\\Text Mining\\Labs\\Lab_skip_gram\\dictionary' + demographicType + '.gensim')\n",
    " max_tif=0\n",
    " num=0\n",
    " count=0\n",
    " topics = ldamodel.print_topics(num_words=5)\n",
    " tops = 0\n",
    " for n, topic in enumerate(topics):\n",
    "  print(\"Cluster NUmber:\", n)\n",
    "  # print(topic[1][1])\n",
    "  for i, val in enumerate(topic[1]):\n",
    "   # print(topic)\n",
    "\n",
    "   if (val == '*'):\n",
    "    top = int(topic[1][i + 2])\n",
    "    # print(top)\n",
    "    for k in range(3, 10):\n",
    "     # print(topic[1][i+k])\n",
    "     if (topic[1][i + k].isdigit()):\n",
    "      top = top * 10 + int(topic[1][i + k])\n",
    "      # print(top)\n",
    "     else:\n",
    "      break\n",
    "    # print(top)\n",
    "    print(dictionary[top])\n",
    "\n",
    "\n",
    "\n",
    " print('Maximum Term Frequency')\n",
    " FindMaxValue(corpus,demographicType)\n",
    " print('Maximum TF-IDF')\n",
    " FindMaxValue(corpus_tfidf,demographicType)\n",
    "\n",
    "#TF-IDF AND LSA MODEL\n",
    "\n",
    " num_topics = n_top\n",
    " corpus = pickle.load(open('D:\\college\\sem2\\Text Mining\\Labs\\Lab_skip_gram\\corpus' + demographicType + '.pkl', 'rb'))\n",
    " lsi = models.LsiModel(corpus_tfidf, num_topics=num_topics)\n",
    " lsi.save('model' + demographicType + '.gensim')\n",
    " dictionary = corpora.Dictionary.load('D:\\college\\sem2\\Text Mining\\Labs\\Lab_skip_gram\\dictionary' + demographicType + '.gensim')\n",
    " topics = lsi.print_topics(num_words=5)\n",
    " tops = []\n",
    " for n, topic in enumerate(topics):\n",
    "  print(\"Cluster NUmber:\", n)\n",
    "  # print(topic[1][1])\n",
    "  for i, val in enumerate(topic[1]):\n",
    "   # print(topic)\n",
    "\n",
    "   if (val == '*'):\n",
    "    top = int(topic[1][i + 2])\n",
    "    # print(top)\n",
    "    for k in range(3, 10):\n",
    "     # print(topic[1][i+k])\n",
    "     if (topic[1][i + k].isdigit()):\n",
    "      top = top * 10 + int(topic[1][i + k])\n",
    "      # print(top)\n",
    "     else:\n",
    "      break\n",
    "    # print(top)\n",
    "    print(dictionary[top])\n",
    " maxTerm = 1\n",
    " num = 0\n",
    " for xyz in corpus:\n",
    "  for abc in xyz:\n",
    "   if (maxTerm < abc[1]):\n",
    "    num = abc[0]\n",
    "    maxTerm = abc[1]\n",
    "\n",
    "\n",
    "# Pass the demography for which you want to run and find the most dominant topic. In this case, most dominant topics of\n",
    "# female blogs have been found\n",
    "# Other Demographies are - Male, Above 20, Below 20 and All.\n",
    "runtfidf(2,\"female\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
